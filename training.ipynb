{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "faca8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.custom_dataset import YOLO6ChannelDataset\n",
    "\n",
    "train_dataset = YOLO6ChannelDataset(\n",
    "    left_img_dir=\"D:/intezet/Bogi/Yolo/data/images/train/left\",\n",
    "    right_img_dir=\"D:/intezet/Bogi/Yolo/data/images/train/right\",\n",
    "    label_dir=\"D:/intezet/Bogi/Yolo/data/labels/train\"\n",
    ")\n",
    "\n",
    "val_dataset = YOLO6ChannelDataset(\n",
    "    left_img_dir=\"D:/intezet/Bogi/Yolo/data/images/val/left\",\n",
    "    right_img_dir=\"D:/intezet/Bogi/Yolo/data/images/val/right\",\n",
    "    label_dir=\"D:/intezet/Bogi/Yolo/data/labels/val\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f721722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       896  ultralytics.nn.modules.conv.Conv             [6, 16, 3, 2]                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752482  ultralytics.nn.modules.head.Detect           [6, [64, 128, 256]]           \n",
      "YOLOv8n summary: 129 layers, 3,012,450 parameters, 3,012,434 gradients, 8.3 GFLOPs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [06:21<00:00, 22.42s/it, loss=361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Epoch 1 - Train Loss: 482.9622\n",
      "ðŸ§ª Epoch 1 - Val Loss: 423.1375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [06:55<00:00, 24.42s/it, loss=338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“˜ Epoch 2 - Train Loss: 448.5152\n",
      "ðŸ§ª Epoch 2 - Val Loss: 422.4111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50:  18%|â–ˆâ–Š        | 3/17 [01:29<06:58, 29.93s/it, loss=437]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m total_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m     52\u001b[39m pbar = tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# print(f\"imgs.shape: {imgs.shape}\")\u001b[39;49;00m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\intezet\\Bogi\\yolo-venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\intezet\\Bogi\\yolo-venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\intezet\\Bogi\\yolo-venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\intezet\\Bogi\\yolo-venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\intezet\\Bogi\\utils\\custom_dataset.py:56\u001b[39m, in \u001b[36mYOLO6ChannelDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Load and resize images\u001b[39;00m\n\u001b[32m     55\u001b[39m left_img = Image.open(os.path.join(\u001b[38;5;28mself\u001b[39m.left_img_dir, filename)).convert(\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m right_img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mright_img_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mRGB\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m left_img = \u001b[38;5;28mself\u001b[39m.resize(left_img)\n\u001b[32m     58\u001b[39m right_img = \u001b[38;5;28mself\u001b[39m.resize(right_img)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\intezet\\Bogi\\yolo-venv\\Lib\\site-packages\\PIL\\Image.py:982\u001b[39m, in \u001b[36mImage.convert\u001b[39m\u001b[34m(self, mode, matrix, dither, palette, colors)\u001b[39m\n\u001b[32m    979\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mBGR;15\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;16\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;24\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    980\u001b[39m     deprecate(mode, \u001b[32m12\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    984\u001b[39m has_transparency = \u001b[33m\"\u001b[39m\u001b[33mtransparency\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    986\u001b[39m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\intezet\\Bogi\\yolo-venv\\Lib\\site-packages\\PIL\\ImageFile.py:389\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    386\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[32m    388\u001b[39m b = b + s\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m n, err_code = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m0\u001b[39m:\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from ultralytics.nn.tasks import DetectionModel\n",
    "from types import SimpleNamespace\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 50\n",
    "batch_size = 8\n",
    "save_path = \"yolov8_custom6.pt\"\n",
    "\n",
    "\n",
    "# --- Collate Function ---\n",
    "def collate_fn(batch):\n",
    "    imgs, targets = zip(*batch)\n",
    "    imgs = torch.stack(imgs, dim=0)\n",
    "    targets_with_id = []\n",
    "    for i, t in enumerate(targets):\n",
    "        if t.numel() == 0:\n",
    "            continue\n",
    "        img_idx = torch.full((t.shape[0], 1), i, dtype=t.dtype)\n",
    "        targets_with_id.append(torch.cat([img_idx, t], dim=1))\n",
    "    if len(targets_with_id) == 0:\n",
    "        targets_combined = torch.zeros((0, 6))\n",
    "    else:\n",
    "        targets_combined = torch.cat(targets_with_id, dim=0)\n",
    "    # print(f\"targets_combined: {targets_combined}\")\n",
    "    return imgs, targets_combined\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                          collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                        collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "\n",
    "# --- Model ---\n",
    "model = DetectionModel(cfg='ultralytics/cfg/models/v8/yolov8n.yaml', ch=6, nc=6).to(device)\n",
    "model.args = SimpleNamespace(box=7.5, cls=0.5, dfl=1.5, reg_max=15)\n",
    "model.criterion = model.init_criterion()\n",
    "model.to(device)\n",
    "# print(f\"Expected output channels: {(model.args.reg_max + 1) * 4 + model.nc}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    for imgs, targets in pbar:\n",
    "        # print(f\"imgs.shape: {imgs.shape}\")\n",
    "        imgs = imgs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        if targets.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(imgs)\n",
    "\n",
    "        batch = {\n",
    "            \"img\": imgs,\n",
    "            \"batch_idx\": targets[:, 0],\n",
    "            \"cls\": targets[:, 1],\n",
    "            \"bboxes\": targets[:, 2:]\n",
    "        }\n",
    "        \n",
    "        loss, loss_items = model.loss(batch, preds)  # loss is scalar, loss_items is [box, cls, dfl]\n",
    "        loss = loss.sum()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"ðŸ“˜ Epoch {epoch+1} - Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            if targets.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            preds = model(imgs)\n",
    "            batch = {\n",
    "                \"img\": imgs,\n",
    "                \"batch_idx\": targets[:, 0],\n",
    "                \"cls\": targets[:, 1],\n",
    "                \"bboxes\": targets[:, 2:]\n",
    "            }\n",
    "            \n",
    "            loss_out, loss_out_items = model.loss(batch, preds)  # loss is scalar, loss_items is [box, cls, dfl]\n",
    "            loss_out = loss_out.sum()\n",
    "            val_loss += loss_out.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"ðŸ§ª Epoch {epoch+1} - Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # --- Save ---\n",
    "    if (epoch + 1) % 5 == 0 or (epoch + 1) == epochs:\n",
    "        model.save('yolov8_custom6.pt')\n",
    "        print(f\"âœ… Model saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7aea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics.nn.tasks import DetectionModel\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# Configuration\n",
    "weights_path = \"yolov8_custom6.pt\"  # Path to your trained model weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load your 6-channel YOLOv8 model with 6 classes\n",
    "model = DetectionModel(cfg='ultralytics/cfg/models/v8/yolov8.yaml', ch=6, nc=6)\n",
    "model.args = SimpleNamespace(box=7.5, cls=0.5, dfl=1.5, reg_max=15)\n",
    "model.criterion = model.init_criterion()\n",
    "model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64bc9360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as T\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((640, 640)),  # Adjust if your model uses a different size\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "def load_6ch_image(path_left, path_right):\n",
    "    img_right = transform(Image.open(path_right).convert(\"RGB\"))  # (3, H, W)\n",
    "    img_left = transform(Image.open(path_left).convert(\"RGB\"))  # (3, H, W)\n",
    "    return torch.cat([img_right, img_left], dim=0)  # (6, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "687ba775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from ultralytics.utils.ops import non_max_suppression\n",
    "\n",
    "def predict_and_draw(img_path_right, img_path_left, output_path, conf_thresh=0.1, iou_thresh=0.5):\n",
    "    # Load your custom 6-channel image (combine right+left)\n",
    "    img_6ch = load_6ch_image(img_path_right, img_path_left).unsqueeze(0).to(device)  # (1, 6, H, W)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        raw_preds = model(img_6ch)  # raw model outputs, shape: (1, 10, 8400) or similar\n",
    "\n",
    "        # Apply postprocessing - decode + NMS in one step\n",
    "        # Note: Use your model's 'postprocess' method if available, else use NMS directly on raw preds\n",
    "        # Since DetectionModel has no decode/postprocess method, use non_max_suppression here:\n",
    "        preds = non_max_suppression(raw_preds, conf_thres=conf_thresh, iou_thres=iou_thresh, nc=6)[0]  # nc=number classes\n",
    "\n",
    "        print(f\"Detections shape: {preds.shape if preds is not None else 'No detections'}\")\n",
    "        print(preds)\n",
    "\n",
    "    # Load original right image for visualization (where you want boxes)\n",
    "    original = cv2.imread(img_path_right)\n",
    "\n",
    "    if preds is not None and preds.shape[0] > 0:\n",
    "        for *xyxy, conf, cls in preds:\n",
    "            # xyxy are the bounding box corners\n",
    "            label = f\"{int(cls.item())}: {conf.item():.2f}\"\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(original, (int(xyxy[0]), int(xyxy[1])), (int(xyxy[2]), int(xyxy[3])), (0, 255, 0), 2)\n",
    "            # Draw label text\n",
    "            cv2.putText(original, label, (int(xyxy[0]), int(xyxy[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                        (0, 255, 0), 1, cv2.LINE_AA)\n",
    "    else:\n",
    "        print(\"No objects detected\")\n",
    "\n",
    "    # Save image with detections\n",
    "    cv2.imwrite(output_path, original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06719277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detections shape: torch.Size([0, 6])\n",
      "tensor([], size=(0, 6))\n",
      "No objects detected\n",
      "Saved: Yolo/runs/predict_custom_6ch_v8\\Image_5377_pred.png\n",
      "Detections shape: torch.Size([0, 6])\n",
      "tensor([], size=(0, 6))\n",
      "No objects detected\n",
      "Saved: Yolo/runs/predict_custom_6ch_v8\\Image_5387_pred.png\n",
      "Detections shape: torch.Size([0, 6])\n",
      "tensor([], size=(0, 6))\n",
      "No objects detected\n",
      "Saved: Yolo/runs/predict_custom_6ch_v8\\Image_5397_pred.png\n",
      "Detections shape: torch.Size([0, 6])\n",
      "tensor([], size=(0, 6))\n",
      "No objects detected\n",
      "Saved: Yolo/runs/predict_custom_6ch_v8\\Image_5407_pred.png\n",
      "Detections shape: torch.Size([0, 6])\n",
      "tensor([], size=(0, 6))\n",
      "No objects detected\n",
      "Saved: Yolo/runs/predict_custom_6ch_v8\\Image_5417_pred.png\n",
      "Detections shape: torch.Size([0, 6])\n",
      "tensor([], size=(0, 6))\n",
      "No objects detected\n",
      "Saved: Yolo/runs/predict_custom_6ch_v8\\Image_5427_pred.png\n",
      "Detections shape: torch.Size([0, 6])\n",
      "tensor([], size=(0, 6))\n",
      "No objects detected\n",
      "Saved: Yolo/runs/predict_custom_6ch_v8\\Image_5437_pred.png\n",
      "Detections shape: torch.Size([0, 6])\n",
      "tensor([], size=(0, 6))\n",
      "No objects detected\n",
      "Saved: Yolo/runs/predict_custom_6ch_v8\\Image_5448_pred.png\n",
      "Detections shape: torch.Size([0, 6])\n",
      "tensor([], size=(0, 6))\n",
      "No objects detected\n",
      "Saved: Yolo/runs/predict_custom_6ch_v8\\Image_5458_pred.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m left_path = os.path.join(input_folder + \u001b[33m\"\u001b[39m\u001b[33m/left\u001b[39m\u001b[33m\"\u001b[39m, file_name)\n\u001b[32m     10\u001b[39m output_path = os.path.join(output_folder, file_name.replace(\u001b[33m\"\u001b[39m\u001b[33m.png\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_pred.png\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mpredict_and_draw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mpredict_and_draw\u001b[39m\u001b[34m(img_path_right, img_path_left, output_path, conf_thresh, iou_thresh)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict_and_draw\u001b[39m(img_path_right, img_path_left, output_path, conf_thresh=\u001b[32m0.1\u001b[39m, iou_thresh=\u001b[32m0.5\u001b[39m):\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Load your custom 6-channel image (combine right+left)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     img_6ch = \u001b[43mload_6ch_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path_right\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_path_left\u001b[49m\u001b[43m)\u001b[49m.unsqueeze(\u001b[32m0\u001b[39m).to(device)  \u001b[38;5;66;03m# (1, 6, H, W)\u001b[39;00m\n\u001b[32m      9\u001b[39m     model.eval()\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mload_6ch_image\u001b[39m\u001b[34m(path_left, path_right)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_6ch_image\u001b[39m(path_left, path_right):\n\u001b[32m      9\u001b[39m     img_right = transform(Image.open(path_right).convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m))  \u001b[38;5;66;03m# (3, H, W)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     img_left = transform(\u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_left\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRGB\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# (3, H, W)\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat([img_right, img_left], dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\intezet\\Bogi\\yolo-venv\\Lib\\site-packages\\PIL\\Image.py:982\u001b[39m, in \u001b[36mImage.convert\u001b[39m\u001b[34m(self, mode, matrix, dither, palette, colors)\u001b[39m\n\u001b[32m    979\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mBGR;15\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;16\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;24\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    980\u001b[39m     deprecate(mode, \u001b[32m12\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    984\u001b[39m has_transparency = \u001b[33m\"\u001b[39m\u001b[33mtransparency\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    986\u001b[39m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\intezet\\Bogi\\yolo-venv\\Lib\\site-packages\\PIL\\ImageFile.py:389\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    386\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[32m    388\u001b[39m b = b + s\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m n, err_code = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m0\u001b[39m:\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "input_folder = \"Yolo/data/test_for_prediction\"\n",
    "output_folder = \"Yolo/runs/predict_custom_6ch_v8\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "file_names = [f for f in os.listdir(input_folder + \"/right\") if \".png\" in f]\n",
    "\n",
    "for file_name in file_names:\n",
    "    right_path = os.path.join(input_folder + \"/right\", file_name)\n",
    "    left_path = os.path.join(input_folder + \"/left\", file_name)\n",
    "    output_path = os.path.join(output_folder, file_name.replace(\".png\", \"_pred.png\"))\n",
    "\n",
    "    predict_and_draw(right_path, left_path, output_path)\n",
    "    print(f\"Saved: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
