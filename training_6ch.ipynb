{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faca8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.custom_dataset import YOLO6ChannelDataset\n",
    "\n",
    "train_dataset = YOLO6ChannelDataset(\n",
    "    left_img_dir=\"D:/intezet/Bogi/Yolo/data/images/train/left\",\n",
    "    right_img_dir=\"D:/intezet/Bogi/Yolo/data/images/train/right\",\n",
    "    label_dir=\"D:/intezet/Bogi/Yolo/data/labels/train\"\n",
    ")\n",
    "\n",
    "val_dataset = YOLO6ChannelDataset(\n",
    "    left_img_dir=\"D:/intezet/Bogi/Yolo/data/images/val/left\",\n",
    "    right_img_dir=\"D:/intezet/Bogi/Yolo/data/images/val/right\",\n",
    "    label_dir=\"D:/intezet/Bogi/Yolo/data/labels/val\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f721722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.160 available  Update with 'pip install -U ultralytics'\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'\u001b[31m\u001b[1mdataloader\u001b[0m' is not a valid YOLO argument. \n'\u001b[31m\u001b[1mval_dataloader\u001b[0m' is not a valid YOLO argument. \n\n    Arguments received: ['yolo', '--f=c:\\\\Users\\\\etiko\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v3828b9ccdfe2b03f2c85fd307ab23270c974989ba.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of ['segment', 'classify', 'detect', 'pose', 'obb']\n                MODE (required) is one of ['val', 'train', 'predict', 'export', 'track', 'benchmark']\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n\n    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n\n    5. Ultralytics solutions usage\n        yolo solutions count or in ['crop', 'blur', 'workout', 'heatmap', 'isegment', 'visioneye', 'speed', 'queue', 'analytics', 'inference', 'trackzone'] source=\"path/to/video.mp4\"\n\n    6. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n        yolo solutions help\n\n    Docs: https://docs.ultralytics.com\n    Solutions: https://docs.ultralytics.com/solutions/\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n     (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[36m(most recent call last)\u001b[39m:\n",
      "  File \u001b[92md:\\intezet\\Bogi\\yolo-venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3672\u001b[39m in \u001b[95mrun_code\u001b[39m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  Cell \u001b[92mIn[11]\u001b[39m\u001b[92m, line 116\u001b[39m\n    model.train(dataloader=train_loader, val_dataloader=val_loader, epochs=50)\n",
      "  File \u001b[92md:\\intezet\\Bogi\\yolo-venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:791\u001b[39m in \u001b[95mtrain\u001b[39m\n    self.trainer = (trainer or self._smart_load(\"trainer\"))(overrides=args, _callbacks=self.callbacks)\n",
      "  File \u001b[92md:\\intezet\\Bogi\\yolo-venv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:119\u001b[39m in \u001b[95m__init__\u001b[39m\n    self.args = get_cfg(cfg, overrides)\n",
      "  File \u001b[92md:\\intezet\\Bogi\\yolo-venv\\Lib\\site-packages\\ultralytics\\cfg\\__init__.py:305\u001b[39m in \u001b[95mget_cfg\u001b[39m\n    check_dict_alignment(cfg, overrides)\n",
      "\u001b[36m  \u001b[39m\u001b[36mFile \u001b[39m\u001b[32md:\\intezet\\Bogi\\yolo-venv\\Lib\\site-packages\\ultralytics\\cfg\\__init__.py:498\u001b[39m\u001b[36m in \u001b[39m\u001b[35mcheck_dict_alignment\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mraise SyntaxError(string + CLI_HELP_MSG) from e\u001b[39m\n",
      "  \u001b[36mFile \u001b[39m\u001b[32m<string>\u001b[39m\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m '\u001b[31m\u001b[1mdataloader\u001b[0m' is not a valid YOLO argument. \n'\u001b[31m\u001b[1mval_dataloader\u001b[0m' is not a valid YOLO argument. \n\n    Arguments received: ['yolo', '--f=c:\\\\Users\\\\etiko\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v3828b9ccdfe2b03f2c85fd307ab23270c974989ba.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of ['segment', 'classify', 'detect', 'pose', 'obb']\n                MODE (required) is one of ['val', 'train', 'predict', 'export', 'track', 'benchmark']\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n\n    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n\n    5. Ultralytics solutions usage\n        yolo solutions count or in ['crop', 'blur', 'workout', 'heatmap', 'isegment', 'visioneye', 'speed', 'queue', 'analytics', 'inference', 'trackzone'] source=\"path/to/video.mp4\"\n\n    6. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n        yolo solutions help\n\n    Docs: https://docs.ultralytics.com\n    Solutions: https://docs.ultralytics.com/solutions/\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n    \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from ultralytics.nn.tasks import DetectionModel\n",
    "from types import SimpleNamespace\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 50\n",
    "batch_size = 8\n",
    "save_path = \"yolov8_custom6.pt\"\n",
    "\n",
    "\n",
    "# --- Collate Function ---\n",
    "def collate_fn(batch):\n",
    "    imgs, targets = zip(*batch)\n",
    "    imgs = torch.stack(imgs, dim=0)  # shape: (batch, 6, H, W)\n",
    "    # targets is a tuple of tensors [num_boxes_i, 5], keep as list for model.train()\n",
    "    targets = list(targets)\n",
    "    return imgs, targets\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                          collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                        collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "\n",
    "# --- Model ---\n",
    "model = DetectionModel(cfg='ultralytics/cfg/models/v8/yolov8n.yaml', ch=6, nc=6).to(device)\n",
    "model.args = SimpleNamespace(box=7.5, cls=0.5, dfl=1.5, reg_max=15)\n",
    "model.criterion = model.init_criterion()\n",
    "model.to(device)\n",
    "# print(f\"Expected output channels: {(model.args.reg_max + 1) * 4 + model.nc}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    for imgs, targets in pbar:\n",
    "        # print(f\"imgs.shape: {imgs.shape}\")\n",
    "        imgs = imgs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        if targets.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(imgs)\n",
    "\n",
    "        batch = {\n",
    "            \"img\": imgs,\n",
    "            \"batch_idx\": targets[:, 0],\n",
    "            \"cls\": targets[:, 1],\n",
    "            \"bboxes\": targets[:, 2:]\n",
    "        }\n",
    "        \n",
    "        loss, loss_items = model.loss(batch, preds)  # loss is scalar, loss_items is [box, cls, dfl]\n",
    "        loss = loss.sum()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"ðŸ“˜ Epoch {epoch+1} - Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            if targets.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            preds = model(imgs)\n",
    "            batch = {\n",
    "                \"img\": imgs,\n",
    "                \"batch_idx\": targets[:, 0],\n",
    "                \"cls\": targets[:, 1],\n",
    "                \"bboxes\": targets[:, 2:]\n",
    "            }\n",
    "            \n",
    "            loss_out, loss_out_items = model.loss(batch, preds)  # loss is scalar, loss_items is [box, cls, dfl]\n",
    "            loss_out = loss_out.sum()\n",
    "            val_loss += loss_out.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"ðŸ§ª Epoch {epoch+1} - Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # --- Save ---\n",
    "    if (epoch + 1) % 5 == 0 or (epoch + 1) == epochs:\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"âœ… Model saved to: {save_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7aea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics.nn.tasks import DetectionModel\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# Configuration\n",
    "weights_path = \"yolov8_custom6.pt\"  # Path to your trained model weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load your 6-channel YOLOv8 model with 6 classes\n",
    "model = DetectionModel(cfg='ultralytics/cfg/models/v8/yolov8.yaml', ch=6, nc=6)\n",
    "model.args = SimpleNamespace(box=7.5, cls=0.5, dfl=1.5, reg_max=15)\n",
    "model.criterion = model.init_criterion()\n",
    "model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64bc9360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((640, 640)),  # Adjust if your model uses a different size\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "def load_6ch_image(path_left, path_right):\n",
    "    img_right = transform(Image.open(path_right).convert(\"RGB\"))  # (3, H, W)\n",
    "    img_left = transform(Image.open(path_left).convert(\"RGB\"))  # (3, H, W)\n",
    "    return torch.cat([img_right, img_left], dim=0)  # (6, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "687ba775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from ultralytics.nn.tasks import DetectionModel\n",
    "from ultralytics.utils.ops import non_max_suppression\n",
    "\n",
    "# Prediction function\n",
    "def predict_and_draw(img_path_right, img_path_left, output_path, conf_thresh=0.1, iou_thresh=0.65):\n",
    "    # Prepare 6-channel image\n",
    "    img_6ch = load_6ch_image(img_path_right, img_path_left)  # Tensor (6, H, W)\n",
    "    img_6ch = img_6ch.unsqueeze(0).to(device)  # (1, 6, H, W)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        raw_preds = model(img_6ch)[0]  # Get raw output\n",
    "        \n",
    "        # print(\"Raw preds shape:\", raw_preds.shape)  # Expect: [1, N, 6+num_classes]\n",
    "        # print(\"Raw preds min/max:\", raw_preds.min().item(), raw_preds.max().item())\n",
    "        raw_preds[..., 4:] = raw_preds[..., 4:].sigmoid()\n",
    "        \n",
    "        # print(\"Raw preds shape afer sigomid:\", raw_preds.shape)  # Expect: [1, N, 6+num_classes]\n",
    "        # print(\"Raw preds min/max after sigmoid:\", raw_preds.min().item(), raw_preds.max().item())\n",
    "        \n",
    "        preds = non_max_suppression(raw_preds, conf_thres=conf_thresh, iou_thres=iou_thresh, nc=6)[0]\n",
    "        \n",
    "        # print(\"Filtered boxes:\", preds.shape)\n",
    "        # print(preds)\n",
    "\n",
    "\n",
    "    original = cv2.imread(img_path_right)\n",
    "    h, w = original.shape[:2]  # Get image dimensions\n",
    "\n",
    "    if preds is not None and len(preds):\n",
    "        for *xyxy, conf, cls in preds:\n",
    "            # Unpack and scale to original image size\n",
    "            x1, y1, x2, y2 = xyxy\n",
    "            x1 = int(x1 * w)\n",
    "            y1 = int(y1 * h)\n",
    "            x2 = int(x2 * w)\n",
    "            y2 = int(y2 * h)\n",
    "\n",
    "            class_names = ['EUK', 'FC', 'FE ', 'class3', 'class4', 'class5']\n",
    "            label = f\"{class_names[int(cls.item())]}: {conf:.2f}\"\n",
    "\n",
    "            cv2.rectangle(original, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(original, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                        (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    cv2.imwrite(output_path, original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06719277",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [80, 80] at index 0 does not match the shape of the indexed tensor [70, 80, 80] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m left_path = os.path.join(input_folder + \u001b[33m\"\u001b[39m\u001b[33m/left\u001b[39m\u001b[33m\"\u001b[39m, file_name)\n\u001b[32m     13\u001b[39m output_path = os.path.join(output_folder, file_name.replace(\u001b[33m\"\u001b[39m\u001b[33m.png\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_pred.png\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mpredict_and_draw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(file_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m i += \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mpredict_and_draw\u001b[39m\u001b[34m(img_path_right, img_path_left, output_path, conf_thresh, iou_thresh)\u001b[39m\n\u001b[32m     17\u001b[39m     raw_preds[..., \u001b[32m4\u001b[39m:] = raw_preds[..., \u001b[32m4\u001b[39m:].sigmoid()\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# print(\"Raw preds shape afer sigomid:\", raw_preds.shape)  # Expect: [1, N, 6+num_classes]\u001b[39;00m\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# print(\"Raw preds min/max after sigmoid:\", raw_preds.min().item(), raw_preds.max().item())\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     preds = \u001b[43mnon_max_suppression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf_thres\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf_thresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_thres\u001b[49m\u001b[43m=\u001b[49m\u001b[43miou_thresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnc\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# print(\"Filtered boxes:\", preds.shape)\u001b[39;00m\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# print(preds)\u001b[39;00m\n\u001b[32m     28\u001b[39m original = cv2.imread(img_path_right)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\intezet\\Bogi\\yolo-venv\\Lib\\site-packages\\ultralytics\\utils\\ops.py:282\u001b[39m, in \u001b[36mnon_max_suppression\u001b[39m\u001b[34m(prediction, conf_thres, iou_thres, classes, agnostic, multi_label, labels, max_det, nc, max_time_img, max_nms, max_wh, in_place, rotated, end2end, return_idxs)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m xi, (x, xk) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(prediction, xinds)):  \u001b[38;5;66;03m# image index, (preds, preds indices)\u001b[39;00m\n\u001b[32m    279\u001b[39m     \u001b[38;5;66;03m# Apply constraints\u001b[39;00m\n\u001b[32m    280\u001b[39m     \u001b[38;5;66;03m# x[((x[:, 2:4] < min_wh) | (x[:, 2:4] > max_wh)).any(1), 4] = 0  # width-height\u001b[39;00m\n\u001b[32m    281\u001b[39m     filt = xc[xi]  \u001b[38;5;66;03m# confidence\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     x, xk = \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilt\u001b[49m\u001b[43m]\u001b[49m, xk[filt]\n\u001b[32m    284\u001b[39m     \u001b[38;5;66;03m# Cat apriori labels if autolabelling\u001b[39;00m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(labels[xi]) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rotated:\n",
      "\u001b[31mIndexError\u001b[39m: The shape of the mask [80, 80] at index 0 does not match the shape of the indexed tensor [70, 80, 80] at index 0"
     ]
    }
   ],
   "source": [
    "input_folder = \"Yolo/data/test_for_prediction\"\n",
    "output_folder = \"Yolo/runs/predict_custom_6ch_v8\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "file_names = [f for f in os.listdir(input_folder + \"/right\") if \".png\" in f]\n",
    "i = 1\n",
    "\n",
    "\n",
    "for file_name in file_names:\n",
    "    \n",
    "    right_path = os.path.join(input_folder + \"/right\", file_name)\n",
    "    left_path = os.path.join(input_folder + \"/left\", file_name)\n",
    "    output_path = os.path.join(output_folder, file_name.replace(\".png\", \"_pred.png\"))\n",
    "\n",
    "    predict_and_draw(right_path, left_path, output_path)\n",
    "    print(f\"Saved: {output_path} {i}/{len(file_names)}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8n_6ch.yaml')\n",
    "\n",
    "model.train(data='dataset.yaml', epochs=50, batch=8, imgsz=640)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
